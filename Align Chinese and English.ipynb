{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to replicate the code by running a ZH - EN bilingual induction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments with the fastText code, using a ZH - EN task with a combination of LDC and CEDICT ZH - EN dictionaries as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fasttext import FastVector\n",
    "\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "    print('source_matrix.shape target_matrix.shape', source_matrix.shape, target_matrix.shape)\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix[0].transpose(), target_matrix[1])\n",
    "    #There is one error in the ipnyp notebook -> one needs to slice the matrix, since it's a stack of matrices\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors are the 50k top ones from pretrained fastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from ../Georgina/master-thesis/data/wiki.zh.50k.vec\n",
      "reading word vectors from ../Georgina/master-thesis/data/wiki.en.50k.vec\n",
      "-0.026612317462\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "fr_dictionary = FastVector(vector_file='wiki.fr.vec')\n",
    "ru_dictionary = FastVector(vector_file='wiki.ru.vec')\n",
    "\n",
    "fr_vector = fr_dictionary[\"chat\"]\n",
    "ru_vector = ru_dictionary[\"кот\"]\n",
    "print(FastVector.cosine_similarity(fr_vector, ru_vector))\n",
    "'''\n",
    "zh_dictionary = FastVector(vector_file='wiki.zh.50k.vec')\n",
    "en_dictionary = FastVector(vector_file='wiki.en.50k.vec')\n",
    "\n",
    "en_vector = en_dictionary[\"middle\"]\n",
    "zh_vector = zh_dictionary[\"中\"]\n",
    "print(FastVector.cosine_similarity(en_vector, zh_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method does not work for ZH - EN because of the missing alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ala', 'ala'), ('ivan', 'ivan'), ('holden', 'holden'), ('agriculture', 'agriculture'), ('koda', 'koda'), ('isn', 'isn'), ('anderson', 'anderson'), ('trans', 'trans'), ('korean', 'korean'), ('see', 'see')]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "en_words = set(en_dictionary.word2id.keys())\n",
    "zh_words = set(zh_dictionary.word2id.keys())\n",
    "overlap = list(en_words & zh_words)\n",
    "bilingual_dictionary = [(entry, entry) for entry in overlap]\n",
    "print(bilingual_dictionary[:10]) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('乌拉尔', 'Ural'), ('卡尔顿', 'Carleton'), ('孜孜矻矻', 'diligently'), ('攻击性', 'aggressiveness'), ('加勒比', 'Caribbean'), ('蕴藏', 'contain'), ('流线型', 'sleek'), ('蒙茸', 'jumbled'), ('看上', 'favor'), ('爪子', 'paw')]\n",
      "32012\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#LDC and CEDICT shuffled data set\n",
    "trainf = 'ldc-cedict-no-duplicates-shuffled-train-single-words.txt'\n",
    "#Open the training data (bilingual dictionary)\n",
    "def read_dict(dict_file):\n",
    "    return [tuple(line.strip().split()) for line in open(dict_file)]\n",
    "train_data = read_dict(trainf)\n",
    "#These are the source and target words\n",
    "#source_words, target_words = zip(*train_data)\n",
    "print(train_data[:10])\n",
    "print(len(train_data)) #This dictionary is 32012 lines long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ZH - EN dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying the transformation.\n",
      "source_matrix.shape target_matrix.shape (2, 7906, 300) (2, 7906, 300)\n",
      "Transformation was applied.\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying the transformation.\")\n",
    "# form the training matrices\n",
    "#source_matrix, target_matrix = make_training_matrices(\n",
    "#    en_dictionary, zh_dictionary, bilingual_dictionary)\n",
    "\n",
    "#I'm doing a ZH - EN translation task here\n",
    "source_matrix = target_matrix = make_training_matrices(zh_dictionary, en_dictionary, train_data)\n",
    "#print('source_matrix shape: ', type(source_matrix), np.ndim(source_matrix))\n",
    "#print('target_matrix shape: ', type(target_matrix), np.ndim(target_matrix))\n",
    "#print('source_dictionary: ', zh_dictionary.shape)\n",
    "#print('target dictionary: ', en_dictionary.shape)\n",
    "\n",
    "# learn and apply the transformation\n",
    "transform = learn_transformation(source_matrix, target_matrix)\n",
    "zh_dictionary.apply_transform(transform)\n",
    "\n",
    "print(\"Transformation was applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we re-evaluate the similarity of some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.457366518916\n",
      "0.430324895354\n",
      "0.378405948261\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "en_vector = en_dictionary[\"chat\"]\n",
    "zh_vector = ru_dictionary[\"кот\"]\n",
    "print(FastVector.cosine_similarity(fr_vector, ru_vector))\n",
    "'''\n",
    "#check out the similarity of country \n",
    "en_vector = en_dictionary[\"country\"]\n",
    "zh_vector = zh_dictionary[\"国\"]\n",
    "print(FastVector.cosine_similarity(en_vector, zh_vector))\n",
    "\n",
    "#check out the similarity of middle\n",
    "en_vector = en_dictionary[\"middle\"]\n",
    "zh_vector = zh_dictionary[\"中\"]\n",
    "print(FastVector.cosine_similarity(en_vector, zh_vector))\n",
    "\n",
    "#check out the similarity of middle and a random unrelated word\n",
    "en_vector = en_dictionary[\"middle\"]\n",
    "zh_vector = zh_dictionary[\"啥\"]\n",
    "print(FastVector.cosine_similarity(en_vector, zh_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国家\n"
     ]
    }
   ],
   "source": [
    "#Try to translate the word \"country\"\n",
    "en_vector = en_dictionary[\"country\"]\n",
    "print(zh_dictionary.translate_nearest_neighbour(en_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZH - EN translation seems to work for this word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
