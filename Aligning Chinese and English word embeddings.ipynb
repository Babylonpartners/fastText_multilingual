{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word translation for Chinese - English with Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments with the \"fastText_multilingual\" code repo by Babylon Health, using a ZH - EN task with a dictionary provided by Facebook research as training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fasttext import FastVector\n",
    "\n",
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "    print('source_matrix.shape target_matrix.shape', source_matrix.shape, target_matrix.shape)\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix[0].transpose(), target_matrix[1])\n",
    "    #There is one error in the ipnyp notebook -> one needs to slice the matrix, since it's a stack of matrices\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors are the 50k first ones from pretrained fastText embeddings. First, we download the pretrained ones. Switch into your favorite terminal window and execute the following:\n",
    "<br>\n",
    "> curl -Lo wiki.zh.vec https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.zh.vec\n",
    "<br>\n",
    "> curl -Lo wiki.en.vec https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "<br>\n",
    "\n",
    "After downloading the word vectors, perform the following commands:\n",
    "<br>\n",
    "> head -50000 wiki.zh.vec > wiki.zh.50k.vec\n",
    "<br>\n",
    "> head -50000 wiki.en.vec > wiki.en.50k.vec\n",
    "\n",
    "<br>\n",
    "This speeds up computation and puts less data in your RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from ../Georgina/master-thesis/data/wiki.zh.50k.vec\n",
      "reading word vectors from ../Georgina/master-thesis/data/wiki.en.50k.vec\n",
      "-0.026612317462\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "fr_dictionary = FastVector(vector_file='wiki.fr.vec')\n",
    "ru_dictionary = FastVector(vector_file='wiki.ru.vec')\n",
    "\n",
    "fr_vector = fr_dictionary[\"chat\"]\n",
    "ru_vector = ru_dictionary[\"кот\"]\n",
    "print(FastVector.cosine_similarity(fr_vector, ru_vector))\n",
    "'''\n",
    "zh_dictionary = FastVector(vector_file='../Georgina/master-thesis/data/wiki.zh.50k.vec')\n",
    "en_dictionary = FastVector(vector_file='../Georgina/master-thesis/data/wiki.en.50k.vec')\n",
    "\n",
    "en_vector = en_dictionary[\"middle\"]\n",
    "zh_vector = zh_dictionary[\"中\"]\n",
    "print(FastVector.cosine_similarity(en_vector, zh_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the bilingual dictionary training and test data provided by Facebook Research.\n",
    "Switch into bash and execute\n",
    "<br>\n",
    "> curl -Lo zh-en.0-5000.txt https://s3.amazonaws.com/arrival/dictionaries/zh-en.0-5000.txt \n",
    "\n",
    "<br>\n",
    "This is a pretty standard 5k dictionary which is a good size for training our mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('年', 'year'), ('月', 'moon'), ('月', 'months'), ('月', 'month'), ('日', 'day'), ('和', 'and'), ('村', 'village'), ('人', 'man'), ('人', 'people'), ('%', '%')]\n",
      "8891\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#LDC and CEDICT shuffled data set\n",
    "#trainf = '../Georgina/master-thesis/data/zh/ldc-cedict-no-duplicates-shuffled-train-single-words.txt'\n",
    "trainf = 'zh-en.0-5000.txt'\n",
    "#Open the training data (bilingual dictionary)\n",
    "def read_dict(dict_file):\n",
    "    return [tuple(line.strip().split()) for line in open(dict_file)]\n",
    "train_data = read_dict(trainf)\n",
    "#These are the source and target words\n",
    "#source_words, target_words = zip(*train_data)\n",
    "print(train_data[:10])\n",
    "print(len(train_data)) #This dictionary is 32012 lines long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ZH - EN dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying the transformation.\n",
      "source_matrix.shape target_matrix.shape (2, 8719, 300) (2, 8719, 300)\n",
      "Transformation was applied.\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying the transformation.\")\n",
    "# form the training matrices\n",
    "#source_matrix, target_matrix = make_training_matrices(\n",
    "#    en_dictionary, zh_dictionary, bilingual_dictionary)\n",
    "\n",
    "#I'm doing a ZH - EN translation task here\n",
    "source_matrix = target_matrix = make_training_matrices(zh_dictionary, en_dictionary, train_data)\n",
    "#print('source_matrix shape: ', type(source_matrix), np.ndim(source_matrix))\n",
    "#print('target_matrix shape: ', type(target_matrix), np.ndim(target_matrix))\n",
    "#print('source_dictionary: ', zh_dictionary.shape)\n",
    "#print('target dictionary: ', en_dictionary.shape)\n",
    "\n",
    "# learn and apply the transformation\n",
    "transform = learn_transformation(source_matrix, target_matrix)\n",
    "zh_dictionary.apply_transform(transform)\n",
    "\n",
    "print(\"Transformation was applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we re-evaluate the similarity of \"chat\" and \"кот\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.415512686471\n",
      "0.415020491828\n",
      "0.369425876542\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "en_vector = en_dictionary[\"chat\"]\n",
    "zh_vector = ru_dictionary[\"кот\"]\n",
    "print(FastVector.cosine_similarity(fr_vector, ru_vector))\n",
    "'''\n",
    "#check out the similarity of country \n",
    "en_vector = en_dictionary[\"country\"]\n",
    "zh_vector = zh_dictionary[\"国\"]\n",
    "print(FastVector.cosine_similarity(en_vector, zh_vector))\n",
    "\n",
    "#check out the similarity of middle\n",
    "en_vector = en_dictionary[\"middle\"]\n",
    "zh_vector = zh_dictionary[\"中\"]\n",
    "print(FastVector.cosine_similarity(en_vector, zh_vector))\n",
    "\n",
    "#check out the similarity of middle and a random unrelated word\n",
    "en_vector = en_dictionary[\"middle\"]\n",
    "zh_vector = zh_dictionary[\"啥\"]\n",
    "print(FastVector.cosine_similarity(en_vector, zh_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "學習\n"
     ]
    }
   ],
   "source": [
    "#Try to translate the word \"learn\"\n",
    "en_vector = en_dictionary[\"learn\"]\n",
    "print(zh_dictionary.translate_nearest_neighbour(en_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZH - EN translation seems to work for this word. Note that the dictionary contains traditional characters (as used in Taiwan and Hong Kong). The first two words are junk translations. Now, let's export this matrix and evaluate it externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "zh_dictionary.export('transformed-matrix-facebookDict.txt')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get our test dictionary for ZH - EN from fastText. For that we switch back into bash. <br>\n",
    "> curl -Lo zh-en.5000-6500.txt https://s3.amazonaws.com/arrival/dictionaries/zh-en.5000-6500.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('反正', 'anyway'), ('繼承人', 'heir'), ('繼承人', 'heirs'), ('斤', 'catty'), ('導航', 'navigate'), ('導航', 'navigator'), ('導航', 'navigation'), ('導航', 'navigating'), ('鵰', 'eagle'), ('退化', 'degeneration')]\n",
      "2483\n"
     ]
    }
   ],
   "source": [
    "testf = 'zh-en.5000-6500.txt'\n",
    "test_data = read_dict(testf)\n",
    "#These are the source and target words\n",
    "#source_words, target_words = zip(*train_data)\n",
    "print(test_data[:10])\n",
    "print(len(test_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading it in, let's start evaluating. Simply loop over all words in our test dictionary, compute the nearest neighbor and count how many have been translated right.<br>\n",
    "This implementation is quite slow, I guess it could be speeded up with a better computation of the nearest neighbor search (think FAISS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n",
      "prec@1 ZH - EN:  0.2857142857142857\n",
      "Size of test vocabulary:  91\n"
     ]
    }
   ],
   "source": [
    "print('Starting evaluation')\n",
    "#n = len(test_data) #vocabulary size\n",
    "n = 0 #vocabulary size\n",
    "prec1_cnt = 0 #how many words we got right so far\n",
    "for c,e in test_data[:100]: #c is the Chinese word, e is the English word\n",
    "    if e in en_dictionary:\n",
    "        #print(e)\n",
    "        n += 1\n",
    "        en_vector = en_dictionary[e] #do we have a word vector for this word?\n",
    "        translation = zh_dictionary.translate_nearest_neighbour(en_vector)\n",
    "        if translation == c: #hooray, correct translation!\n",
    "            prec1_cnt += 1\n",
    "print('prec@1 ZH - EN: ', prec1_cnt / n)\n",
    "print('Size of test vocabulary: ', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! For speed reasons, we test here only with the first 100 entries, but you can of course compute the whole test set performance if you have time. It's pretty good for such a simple method. Keep in mind that we only used a vocabulary size of 50k in our word embeddings so some words that are in our test set probably weren't found in our test dictionary. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
